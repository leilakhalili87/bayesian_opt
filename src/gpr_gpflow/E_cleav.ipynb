{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numerous-cedar",
   "metadata": {},
   "source": [
    "# Noise-free optimization with Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import trieste\n",
    "\n",
    "import gpflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from gpflow.utilities import print_summary, positive, set_trainable\n",
    "from gpflow.config import set_default_float, default_float, set_default_summary_fmt\n",
    "from gpflow.ci_utils import ci_niter\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "# define database '297' or '388'\n",
    "N = 297\n",
    "\n",
    "# define numbe rof training dataset\n",
    "n_train = 200\n",
    "\n",
    "n_iter = 1\n",
    "\n",
    "n_max = 1\n",
    "# Read the data\n",
    "if N == 297:\n",
    "    data_E = '../../../data/297_energy.txt'\n",
    "    data_pd = '../../../data/297_octonion_pd.txt'\n",
    "    data_axes = '../../../data/sigma3_data.txt'\n",
    "    if n_train > N:\n",
    "        print('The numbe rof trainign datset should be smaller than 297')\n",
    "else:\n",
    "    data_E = '../../../data/energy_olms.txt'\n",
    "    data_pd = '../../../data/pd_olms.txt'\n",
    "    if n_train > N:\n",
    "        print('The numbe rof trainign datset should be smaller than 388')\n",
    "def _octonion_dist(X, X2):\n",
    "    X = tf.reshape(X, [-1,1])\n",
    "    pd = np.loadtxt(data_pd)\n",
    "    X2 = tf.reshape(X2, [-1,1])\n",
    "    dist0 = np.zeros((len(X), len(X2)))\n",
    "    dist = tf.Variable(dist0) # Use variable \n",
    "    for i in range(len(X)):\n",
    "        init_val = int(X[i].numpy())\n",
    "        for j in range(len(X2)):\n",
    "            fin_val = int(X2[j].numpy())\n",
    "            dist0[i,j] = pd[init_val, fin_val]\n",
    "    dist.assign(dist0)\n",
    "    return dist\n",
    "\n",
    "def _octonion_dist_single(X):\n",
    "    X = tf.reshape(X, [-1,1])\n",
    "    pd = np.loadtxt(data_pd)\n",
    "    dist0 = np.zeros((len(X)))\n",
    "    dist = tf.Variable(dist0) # Use variable \n",
    "    for i in range(len(X)):\n",
    "        init_val = int(X[i].numpy())\n",
    "\n",
    "        dist0[i] = pd[init_val, init_val]\n",
    "    dist.assign(dist0)\n",
    "    return dist\n",
    "\n",
    "class GBKernel(gpflow.kernels.Kernel):\n",
    "    def __init__(self, variance=1, lengthscales=.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.variance = gpflow.Parameter(variance, transform=positive())\n",
    "        self.lengthscales = gpflow.Parameter(lengthscales, transform=positive())\n",
    "        self._validate_ard_active_dims(self.lengthscales)\n",
    "    \n",
    "    def K(self, X, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "        a = self.variance * tf.exp(-_octonion_dist(X, X2)/self.lengthscales)\n",
    "        tf.debugging.check_numerics(self.lengthscales, 'hi')\n",
    "        return a\n",
    "\n",
    "    def K_diag(self, X):\n",
    "        a = self.variance * tf.exp(-_octonion_dist_single(X)/self.lengthscales)\n",
    "        tf.debugging.check_numerics(self.lengthscales, 'hi')\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-seller",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#input data\n",
    "N = 297\n",
    "id_gb = np.linspace(0,N-1,N)\n",
    "id_gb = tf.convert_to_tensor(id_gb, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-sailing",
   "metadata": {},
   "source": [
    "## Describe the problem\n",
    "In this example, we look to find the minimum value of the two-dimensional Branin function over the hypercube $[0, 1]^2$. We can represent the search space using a `Box`, and plot contours of the Branin over this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "def gb_func(id_gb):\n",
    "    \"\"\"\n",
    "    x is the id of gbs\n",
    "    output is the energy\n",
    "    \"\"\"\n",
    "    y0 = -np.loadtxt('../../../data/E_cleav.txt')\n",
    "#     y0 = -np.loadtxt('../../../data/energy_olms.txt')\n",
    "    mean_y = np.mean(y0)\n",
    "    std_y = np.sqrt(np.var(y0))\n",
    "    y = (y0 - mean_y) / std_y\n",
    "    id_gb = id_gb.numpy()\n",
    "    Y = y[id_gb.astype(int)]\n",
    "    return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-police",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "input_data =tf.convert_to_tensor(id_gb)\n",
    "inp = tf.reshape(input_data, [-1,1])\n",
    "search_space = trieste.space.DiscreteSearchSpace(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-dealer",
   "metadata": {},
   "source": [
    "## Sample the observer over the search space\n",
    "\n",
    "Sometimes we don't have direct access to the objective function. We only have an observer that indirectly observes it. In _Trieste_, the observer outputs a number of datasets, each of which must be labelled so the optimization process knows which is which. In our case, we only have one dataset, the objective. We'll use _Trieste_'s default label for single-model setups, `OBJECTIVE`. We can convert a function with `branin`'s signature to a single-output observer using `mk_observer`.\n",
    "\n",
    "The optimization procedure will benefit from having some starting data from the objective function to base its search on. We sample five points from the search space and evaluate them on the observer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-arkansas",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from trieste.acquisition.rule import OBJECTIVE\n",
    "\n",
    "observer = trieste.utils.objectives.mk_observer(gb_func, OBJECTIVE)\n",
    "\n",
    "num_initial_points = 2\n",
    "initial_query_points = np.array([18, 209])\n",
    "# initial_query_points = search_space.sample(num_initial_points)\n",
    "initial_query_points = tf.convert_to_tensor(initial_query_points, dtype=tf.float64)\n",
    "initial_query_points = tf.reshape(initial_query_points, [-1,1])\n",
    "\n",
    "initial_data = observer(initial_query_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_query_points.shape\n",
    "# initial_query_points0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-winter",
   "metadata": {},
   "source": [
    "## Model the objective function\n",
    "\n",
    "The Bayesian optimization procedure estimates the next best points to query by using a probabilistic model of the objective. We'll use Gaussian process regression for this, provided by GPflow. The model will need to be trained on each step as more points are evaluated, so we'll package it with GPflow's Scipy optimizer.\n",
    "\n",
    "Just like the data output by the observer, the optimization process assumes multiple models, so we'll need to label the model in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "\n",
    "def build_model(data):\n",
    "    variance = tf.math.reduce_variance(data.observations)\n",
    "    kernel = GBKernel()\n",
    "    gpr = gpflow.models.GPR(data.astuple(), kernel, noise_variance=1e-5)\n",
    "#     print_summary(gpr)\n",
    "    gpflow.set_trainable(gpr.kernel.lengthscales, True)\n",
    "    gpflow.set_trainable(gpr.kernel.variance, True)\n",
    "    gpflow.set_trainable(gpr.likelihood, False)\n",
    "\n",
    "    return {OBJECTIVE: {\n",
    "        \"model\": gpr,\n",
    "        \"optimizer\": gpflow.optimizers.Scipy(),\n",
    "        \"optimizer_args\": {\n",
    "            \"minimize_args\": {\"options\": dict(maxiter=100)},\n",
    "        },\n",
    "    }}\n",
    "\n",
    "model = build_model(initial_data[OBJECTIVE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-professional",
   "metadata": {},
   "source": [
    "## Run the optimization loop\n",
    "\n",
    "We can now run the Bayesian optimization loop by defining a `BayesianOptimizer` and calling its `optimize` method.\n",
    "\n",
    "The optimizer uses an acquisition rule to choose where in the search space to try on each optimization step. We'll use the default acquisition rule, which is Efficient Global Optimization with Expected Improvement.\n",
    "\n",
    "We'll run the optimizer for fifteen steps.\n",
    "\n",
    "The optimization loop catches errors so as not to lose progress, which means the optimization loop might not complete and the data from the last step may not exist. Here we'll handle this crudely by asking for the data regardless, using `.try_get_final_datasets()`, which will re-raise the error if one did occur. For a review of how to handle errors systematically, there is a [dedicated tutorial](recovering_from_errors.ipynb). Finally, like the observer, the optimizer outputs labelled datasets, so we'll get the (only) dataset here by indexing with tag `OBJECTIVE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = trieste.bayesian_optimizer.BayesianOptimizer(observer, search_space)\n",
    "# print_summary(model)\n",
    "result = bo.optimize(50, initial_data, model)\n",
    "dataset = result.try_get_final_datasets()[OBJECTIVE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-width",
   "metadata": {},
   "source": [
    "## Explore the results\n",
    "\n",
    "We can now get the best point found by the optimizer. Note this isn't necessarily the point that was last evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = dataset.query_points.numpy()\n",
    "observations = dataset.observations.numpy()\n",
    "y0 = -np.loadtxt('../../../data/E_cleav.txt')\n",
    "mean_y = np.mean(y0)\n",
    "std_y = np.sqrt(np.var(y0))\n",
    "\n",
    "predicted = (observations * std_y + mean_y)\n",
    "arg_min_idx = tf.squeeze(tf.argmin(predicted, axis=0))\n",
    "\n",
    "print(f\"grain boundary id: {int(query_points[arg_min_idx, :]) + 1}\")\n",
    "print(f\"Predicted value: {-predicted[arg_min_idx, :]}\")\n",
    "print(f\"Optimization step: {arg_min_idx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from util.plotting import plot_regret\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plot_regret(predicted, ax, num_init=num_initial_points, idx_best=arg_min_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_gb = np.linspace(0,N-1,N)\n",
    "# plt.scatter(id_gb, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-grenada",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "ls_list = [\n",
    "    step.models[OBJECTIVE].model.kernel.lengthscales.numpy()  # type: ignore\n",
    "    for step in result.history + [result.final_result.unwrap()]\n",
    "]\n",
    "\n",
    "var_list = [\n",
    "    step.models[OBJECTIVE].model.kernel.variance.numpy()  # type: ignore\n",
    "    for step in result.history + [result.final_result.unwrap()]\n",
    "]\n",
    "ls = np.array(ls_list)\n",
    "var = np.array(var_list)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "fig.suptitle('Horizontally stacked subplots')\n",
    "ax1.plot(ls)\n",
    "ax1.set_xlabel('step')\n",
    "ax1.set_ylabel('lengthscale')\n",
    "ax2.plot(var)\n",
    "ax2.set_xlabel('step')\n",
    "ax2.set_ylabel('variance')\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "gpflow.utilities.print_summary(result.try_get_final_models()[OBJECTIVE].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
